{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "My_lstm_seq2seq.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DarksterTwilight/Eng_to_French-RNN-/blob/main/My_lstm_seq2seq.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LxBFNK2i5mUu"
      },
      "source": [
        "# Character-level recurrent sequence-to-sequence model\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sad_w8Pg5mUx"
      },
      "source": [
        "## Introduction\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jP8N-2pY5mUx"
      },
      "source": [
        "## Setup\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HbpP4q2i5mUy"
      },
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IPDQDMPv5mUy"
      },
      "source": [
        "## Download the data\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9vlV3B635mUz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dfca419e-6df0-4156-854c-e55ae3670840"
      },
      "source": [
        "!!curl -O http://www.manythings.org/anki/fra-eng.zip\n",
        "!!unzip fra-eng.zip\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Archive:  fra-eng.zip',\n",
              " '  inflating: _about.txt              ',\n",
              " '  inflating: fra.txt                 ']"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ykVdIUpg5mUz"
      },
      "source": [
        "## Configuration\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ev6LlNNm5mUz"
      },
      "source": [
        "batch_size = 64                      # Batch size for training.\n",
        "epochs = 100                         # Number of epochs to train for.\n",
        "latent_dim = 256                     # Latent dimensionality of the encoding space.\n",
        "num_samples = 10000                  # Number of samples to train on.\n",
        "data_path = \"fra.txt\"                # Path to the data txt file on disk.\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BLW7oVNc5mU0"
      },
      "source": [
        "## Prepare the data\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pb4By3b5MHlq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2325cf5e-f59a-42e7-873b-5799f234704f"
      },
      "source": [
        "# Vectorize the data.\n",
        "input_texts = []\n",
        "target_texts = []\n",
        "input_characters = set()\n",
        "target_characters = set()\n",
        "print(type(input_characters))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'set'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uPvPYSRYMQK0",
        "outputId": "13a29178-f10c-4d1c-823f-89951d20bd00"
      },
      "source": [
        "with open(data_path, \"r\", encoding=\"utf-8\") as f:\n",
        "    lines = f.read().split(\"\\n\")                    #code to open and read data from a File (provided by Python3)\n",
        "\n",
        "total_lines = len(lines)\n",
        "if num_samples >= total_lines:\n",
        "    start_sample = 0\n",
        "else:\n",
        "    start_sample = total_lines - num_samples - 1    #code to select last 10000 samples from the Corpus\n",
        "\n",
        "for line in lines[start_sample: len(lines) - 1]:\n",
        "    input_text, target_text, _ = line.split(\"\\t\")\n",
        "    target_text = \"\\t\" + target_text + \"\\n\"         # We use \"tab\" as the \"start sequence\" character for the targets, and \"\\n\" as \"end sequence\" character.\n",
        "    input_texts.append(input_text)\n",
        "    target_texts.append(target_text)\n",
        "    for char in input_text:\n",
        "        if char not in input_characters:\n",
        "            input_characters.add(char)\n",
        "    for char in target_text:\n",
        "        if char not in target_characters:\n",
        "            target_characters.add(char)\n",
        "\n",
        "#print(type(input_characters))\n",
        "\n",
        "input_characters = sorted(list(input_characters))\n",
        "target_characters = sorted(list(target_characters))\n",
        "num_encoder_tokens = len(input_characters) + 1 #pad character\n",
        "num_decoder_tokens = len(target_characters) + 1 #pad character\n",
        "max_encoder_seq_length = max([len(txt) for txt in input_texts])\n",
        "max_decoder_seq_length = max([len(txt) for txt in target_texts])\n",
        "\n",
        "#print(type(input_characters))\n",
        "\n",
        "print(\"Number of samples:\", len(input_texts))\n",
        "print(\"Number of unique input tokens:\", num_encoder_tokens)\n",
        "print(\"Number of unique output tokens:\", num_decoder_tokens)\n",
        "print(\"Max sequence length for inputs:\", max_encoder_seq_length)\n",
        "print(\"Max sequence length for outputs:\", max_decoder_seq_length)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'set'>\n",
            "<class 'list'>\n",
            "Number of samples: 10000\n",
            "Number of unique input tokens: 80\n",
            "Number of unique output tokens: 103\n",
            "Max sequence length for inputs: 286\n",
            "Max sequence length for outputs: 351\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cc63qwCpMY09",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3f5e2773-1369-4a9d-8169-966176423da9"
      },
      "source": [
        "input_token_index = dict([(char, i+1) for i, char in enumerate(input_characters)])\n",
        "target_token_index = dict([(char, i+1) for i, char in enumerate(target_characters)])\n",
        "print(input_token_index)\n",
        "print(target_token_index)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{' ': 1, '!': 2, '\"': 3, '$': 4, '%': 5, \"'\": 6, '+': 7, ',': 8, '-': 9, '.': 10, '0': 11, '1': 12, '2': 13, '3': 14, '4': 15, '5': 16, '6': 17, '7': 18, '8': 19, '9': 20, ':': 21, ';': 22, '?': 23, 'A': 24, 'B': 25, 'C': 26, 'D': 27, 'E': 28, 'F': 29, 'G': 30, 'H': 31, 'I': 32, 'J': 33, 'K': 34, 'L': 35, 'M': 36, 'N': 37, 'O': 38, 'P': 39, 'Q': 40, 'R': 41, 'S': 42, 'T': 43, 'U': 44, 'V': 45, 'W': 46, 'X': 47, 'Y': 48, 'Z': 49, 'a': 50, 'b': 51, 'c': 52, 'd': 53, 'e': 54, 'f': 55, 'g': 56, 'h': 57, 'i': 58, 'j': 59, 'k': 60, 'l': 61, 'm': 62, 'n': 63, 'o': 64, 'p': 65, 'q': 66, 'r': 67, 's': 68, 't': 69, 'u': 70, 'v': 71, 'w': 72, 'x': 73, 'y': 74, 'z': 75, '\\xa0': 76, 'é': 77, '—': 78, '’': 79}\n",
            "{'\\t': 1, '\\n': 2, ' ': 3, '!': 4, '\"': 5, '%': 6, \"'\": 7, '+': 8, ',': 9, '-': 10, '.': 11, '/': 12, '0': 13, '1': 14, '2': 15, '3': 16, '4': 17, '5': 18, '6': 19, '7': 20, '8': 21, '9': 22, ':': 23, ';': 24, '?': 25, 'A': 26, 'B': 27, 'C': 28, 'D': 29, 'E': 30, 'F': 31, 'G': 32, 'H': 33, 'I': 34, 'J': 35, 'K': 36, 'L': 37, 'M': 38, 'N': 39, 'O': 40, 'P': 41, 'Q': 42, 'R': 43, 'S': 44, 'T': 45, 'U': 46, 'V': 47, 'W': 48, 'X': 49, 'Y': 50, 'Z': 51, 'a': 52, 'b': 53, 'c': 54, 'd': 55, 'e': 56, 'f': 57, 'g': 58, 'h': 59, 'i': 60, 'j': 61, 'k': 62, 'l': 63, 'm': 64, 'n': 65, 'o': 66, 'p': 67, 'q': 68, 'r': 69, 's': 70, 't': 71, 'u': 72, 'v': 73, 'w': 74, 'x': 75, 'y': 76, 'z': 77, '\\xa0': 78, '«': 79, '»': 80, 'À': 81, 'Â': 82, 'Ç': 83, 'É': 84, 'Ê': 85, 'à': 86, 'á': 87, 'â': 88, 'ç': 89, 'è': 90, 'é': 91, 'ê': 92, 'ë': 93, 'î': 94, 'ï': 95, 'ô': 96, 'ù': 97, 'û': 98, 'œ': 99, '\\u2009': 100, '’': 101, '\\u202f': 102}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j9EPa98BMoKg",
        "outputId": "0db52ae3-4579-4e09-a123-7fc60aec13b2"
      },
      "source": [
        "print('Integer Value of A is: '+str(input_token_index['A']))\n",
        "print('Integer Value of B is: '+str(input_token_index['B']))\n",
        "print('Integer Value of space is: '+str(input_token_index[' ']))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Integer Value of A is: 24\n",
            "Integer Value of B is: 25\n",
            "Integer Value of space is: 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iW_AuM6PN15j"
      },
      "source": [
        "input_token_index['pad'] = 0\n",
        "target_token_index['pad'] = 0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q15IEeYdMkuq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "19c6beca-1704-4123-ac03-1151ff688b56"
      },
      "source": [
        "encoder_input_data = np.zeros(\n",
        "    (len(input_texts), max_encoder_seq_length), dtype=\"float32\"             #shapes of numpy arrays for storing data\n",
        ")     # 10,000   ,   286\n",
        "decoder_input_data = np.zeros(\n",
        "    (len(input_texts), max_decoder_seq_length), dtype=\"float32\"\n",
        ")    #    10000    , 351\n",
        "decoder_target_data = np.zeros(\n",
        "    (len(input_texts), max_decoder_seq_length, num_decoder_tokens), dtype=\"float32\"\n",
        ")    #   10000    ,   351   , 103\n",
        "\n",
        "print(type(encoder_input_data))\n",
        "print(type(decoder_input_data))\n",
        "print(type(decoder_target_data))\n",
        "\n",
        "print(np.shape(encoder_input_data))\n",
        "print(np.shape(decoder_input_data))\n",
        "print(np.shape(decoder_target_data))\n",
        "\n",
        "for i, (input_text, target_text) in enumerate(zip(input_texts, target_texts)):\n",
        "    for t, char in enumerate(input_text):\n",
        "        encoder_input_data[i, t] = input_token_index[char]                      # Save the value corresponding to the \"char\" at position i, t\n",
        "    encoder_input_data[i, t + 1 :] = input_token_index['pad']\n",
        "    \n",
        "    for t, char in enumerate(target_text):\n",
        "        decoder_input_data[i, t] = target_token_index[char]                     # decoder_target_data is ahead of decoder_input_data by one timestep\n",
        "        if t > 0:\n",
        "            decoder_target_data[i, t - 1, target_token_index[char]] = 1.0       # decoder_target_data will be ahead by one timestep and will not include the start character.\n",
        "    decoder_input_data[i, t + 1 :] = target_token_index['pad']\n",
        "    decoder_target_data[i, t:, target_token_index[\" \"]] = 1.0"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'numpy.ndarray'>\n",
            "<class 'numpy.ndarray'>\n",
            "<class 'numpy.ndarray'>\n",
            "(10000, 286)\n",
            "(10000, 351)\n",
            "(10000, 351, 103)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_w3pToeV5mU1"
      },
      "source": [
        "## Build the model\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v9O_wynSXazS"
      },
      "source": [
        "class Encoder(tf.keras.Model):\n",
        "    def __init__(self, embed_dim, num_encoder_tokens):\n",
        "        super(Encoder, self).__init__()\n",
        "        \n",
        "        self.embedding_layer = tf.keras.layers.Embedding(embed_dim, num_encoder_tokens, mask_zero=True)  \n",
        "            # bs , seq_len, emb_dim\n",
        "        self.lstm_layer = tf.keras.layers.LSTM(embed_dim, return_state=True)\n",
        "\n",
        "    def call(self, input, training = True):\n",
        "        encoder_embeddings = self.embedding_layer(input)\n",
        "        encoder_outputs, state_h, state_c = self.lstm_layer(encoder_embeddings)   # bs, emb dim  # bs, emb dim   # bs, emb dim\n",
        "        encoder_states = [state_h, state_c]                                       # List of Encoder States\n",
        "        return encoder_outputs, encoder_states"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XrDUsXotY9eo"
      },
      "source": [
        "class Decoder(tf.keras.Model):\n",
        "    def __init__(self, embed_dim, num_decoder_tokens):\n",
        "        super(Decoder, self).__init__()\n",
        "        \n",
        "        self.embedding_layer = tf.keras.layers.Embedding(embed_dim, num_decoder_tokens, mask_zero=True)\n",
        "        self.lstm_layer = keras.layers.LSTM(embed_dim, return_sequences=True, return_state=True)\n",
        "        self.dense = keras.layers.Dense(num_decoder_tokens)\n",
        "\n",
        "    def call(self, inputs, training = True):\n",
        "        text_input = inputs[0]\n",
        "        state_input = inputs[1]\n",
        "\n",
        "        text_embedding = self.embedding_layer(text_input)\n",
        "\n",
        "        decoder_ouputs, state_h, state_c = self.lstm_layer(text_embedding, initial_state=state_input)\n",
        "        decoder_states = [state_h, state_c]\n",
        "\n",
        "        decoder_op = self.dense(decoder_ouputs)\n",
        "\n",
        "        return decoder_op, decoder_states\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ElMNmbwqZjYU"
      },
      "source": [
        "class training_model(tf.keras.Model):\n",
        "    def __init__(self, encoder_model, decoder_model):\n",
        "        super(training_model, self).__init__()\n",
        "        \n",
        "        self.encoder = encoder_model\n",
        "        self.decoder = decoder_model\n",
        "\n",
        "    def call(self, inputs, training = True):\n",
        "        encoder_text = inputs[0]\n",
        "        decoder_text = inputs[1]\n",
        "\n",
        "        encoder_op, encoder_state_op = self.encoder(encoder_text, training = training)\n",
        "\n",
        "        decoder_op, _ = self.decoder([decoder_text, encoder_state_op], training = training)\n",
        "\n",
        "        return decoder_op"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "crjO8Sn4f_hE"
      },
      "source": [
        "encoder_model = Encoder(latent_dim, num_encoder_tokens)\n",
        "decoder_model = Decoder(latent_dim,  num_decoder_tokens)\n",
        "model = training_model(encoder_model, decoder_model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FPrey_b95mU2"
      },
      "source": [
        "## Train the model\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9grrm7XG5mU2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "51c17800-bbd1-44a9-f073-4385a41869d8"
      },
      "source": [
        "model.compile(\n",
        "    optimizer=\"rmsprop\", loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True), metrics=[\"accuracy\"]\n",
        ")\n",
        "model.fit(\n",
        "    [encoder_input_data, decoder_input_data],\n",
        "    decoder_target_data,\n",
        "    batch_size=batch_size,\n",
        "    epochs=epochs,\n",
        "    validation_split=0.2,\n",
        ")\n",
        "# Save model\n",
        "model.save(\"s2s\")\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "125/125 [==============================] - 11s 51ms/step - loss: 0.5520 - accuracy: 0.2396 - val_loss: 0.6732 - val_accuracy: 0.3147\n",
            "Epoch 2/100\n",
            "125/125 [==============================] - 5s 39ms/step - loss: 0.4398 - accuracy: 0.3544 - val_loss: 0.6075 - val_accuracy: 0.3700\n",
            "Epoch 3/100\n",
            "125/125 [==============================] - 5s 39ms/step - loss: 0.4031 - accuracy: 0.3989 - val_loss: 0.5641 - val_accuracy: 0.4007\n",
            "Epoch 4/100\n",
            "125/125 [==============================] - 5s 39ms/step - loss: 0.3758 - accuracy: 0.4322 - val_loss: 0.5338 - val_accuracy: 0.4284\n",
            "Epoch 5/100\n",
            "125/125 [==============================] - 5s 39ms/step - loss: 0.3522 - accuracy: 0.4639 - val_loss: 0.5004 - val_accuracy: 0.4621\n",
            "Epoch 6/100\n",
            "125/125 [==============================] - 5s 39ms/step - loss: 0.3322 - accuracy: 0.4932 - val_loss: 0.4832 - val_accuracy: 0.4832\n",
            "Epoch 7/100\n",
            "125/125 [==============================] - 5s 39ms/step - loss: 0.3158 - accuracy: 0.5181 - val_loss: 0.4675 - val_accuracy: 0.4995\n",
            "Epoch 8/100\n",
            "125/125 [==============================] - 5s 40ms/step - loss: 0.3026 - accuracy: 0.5387 - val_loss: 0.4448 - val_accuracy: 0.5223\n",
            "Epoch 9/100\n",
            "125/125 [==============================] - 5s 39ms/step - loss: 0.2895 - accuracy: 0.5583 - val_loss: 0.4299 - val_accuracy: 0.5358\n",
            "Epoch 10/100\n",
            "125/125 [==============================] - 5s 39ms/step - loss: 0.2785 - accuracy: 0.5751 - val_loss: 0.4189 - val_accuracy: 0.5492\n",
            "Epoch 11/100\n",
            "125/125 [==============================] - 5s 41ms/step - loss: 0.2698 - accuracy: 0.5881 - val_loss: 0.4066 - val_accuracy: 0.5632\n",
            "Epoch 12/100\n",
            "125/125 [==============================] - 5s 39ms/step - loss: 0.2620 - accuracy: 0.5990 - val_loss: 0.4007 - val_accuracy: 0.5671\n",
            "Epoch 13/100\n",
            "125/125 [==============================] - 5s 39ms/step - loss: 0.2552 - accuracy: 0.6087 - val_loss: 0.3926 - val_accuracy: 0.5783\n",
            "Epoch 14/100\n",
            "125/125 [==============================] - 5s 39ms/step - loss: 0.2491 - accuracy: 0.6175 - val_loss: 0.3884 - val_accuracy: 0.5808\n",
            "Epoch 15/100\n",
            "125/125 [==============================] - 5s 39ms/step - loss: 0.2437 - accuracy: 0.6248 - val_loss: 0.3794 - val_accuracy: 0.5902\n",
            "Epoch 16/100\n",
            "125/125 [==============================] - 5s 39ms/step - loss: 0.2385 - accuracy: 0.6323 - val_loss: 0.3729 - val_accuracy: 0.5989\n",
            "Epoch 17/100\n",
            "125/125 [==============================] - 5s 39ms/step - loss: 0.2339 - accuracy: 0.6388 - val_loss: 0.3714 - val_accuracy: 0.5990\n",
            "Epoch 18/100\n",
            "125/125 [==============================] - 5s 39ms/step - loss: 0.2297 - accuracy: 0.6450 - val_loss: 0.3659 - val_accuracy: 0.6043\n",
            "Epoch 19/100\n",
            "125/125 [==============================] - 5s 40ms/step - loss: 0.2257 - accuracy: 0.6507 - val_loss: 0.3668 - val_accuracy: 0.6022\n",
            "Epoch 20/100\n",
            "125/125 [==============================] - 5s 40ms/step - loss: 0.2222 - accuracy: 0.6564 - val_loss: 0.3615 - val_accuracy: 0.6094\n",
            "Epoch 21/100\n",
            "125/125 [==============================] - 5s 40ms/step - loss: 0.2187 - accuracy: 0.6613 - val_loss: 0.3621 - val_accuracy: 0.6072\n",
            "Epoch 22/100\n",
            "125/125 [==============================] - 5s 40ms/step - loss: 0.2157 - accuracy: 0.6658 - val_loss: 0.3575 - val_accuracy: 0.6125\n",
            "Epoch 23/100\n",
            "125/125 [==============================] - 5s 39ms/step - loss: 0.2126 - accuracy: 0.6704 - val_loss: 0.3577 - val_accuracy: 0.6130\n",
            "Epoch 24/100\n",
            "125/125 [==============================] - 5s 39ms/step - loss: 0.2096 - accuracy: 0.6751 - val_loss: 0.3535 - val_accuracy: 0.6185\n",
            "Epoch 25/100\n",
            "125/125 [==============================] - 5s 39ms/step - loss: 0.2069 - accuracy: 0.6789 - val_loss: 0.3526 - val_accuracy: 0.6195\n",
            "Epoch 26/100\n",
            "125/125 [==============================] - 5s 39ms/step - loss: 0.2043 - accuracy: 0.6827 - val_loss: 0.3522 - val_accuracy: 0.6199\n",
            "Epoch 27/100\n",
            "125/125 [==============================] - 5s 39ms/step - loss: 0.2019 - accuracy: 0.6859 - val_loss: 0.3486 - val_accuracy: 0.6243\n",
            "Epoch 28/100\n",
            "125/125 [==============================] - 5s 39ms/step - loss: 0.1995 - accuracy: 0.6894 - val_loss: 0.3494 - val_accuracy: 0.6230\n",
            "Epoch 29/100\n",
            "125/125 [==============================] - 5s 40ms/step - loss: 0.1971 - accuracy: 0.6930 - val_loss: 0.3457 - val_accuracy: 0.6287\n",
            "Epoch 30/100\n",
            "125/125 [==============================] - 5s 40ms/step - loss: 0.1950 - accuracy: 0.6965 - val_loss: 0.3463 - val_accuracy: 0.6280\n",
            "Epoch 31/100\n",
            "125/125 [==============================] - 5s 39ms/step - loss: 0.1931 - accuracy: 0.6989 - val_loss: 0.3458 - val_accuracy: 0.6298\n",
            "Epoch 32/100\n",
            "125/125 [==============================] - 5s 39ms/step - loss: 0.1909 - accuracy: 0.7028 - val_loss: 0.3454 - val_accuracy: 0.6286\n",
            "Epoch 33/100\n",
            "125/125 [==============================] - 5s 40ms/step - loss: 0.1890 - accuracy: 0.7051 - val_loss: 0.3456 - val_accuracy: 0.6288\n",
            "Epoch 34/100\n",
            "125/125 [==============================] - 5s 40ms/step - loss: 0.1871 - accuracy: 0.7080 - val_loss: 0.3443 - val_accuracy: 0.6313\n",
            "Epoch 35/100\n",
            "125/125 [==============================] - 5s 39ms/step - loss: 0.1853 - accuracy: 0.7106 - val_loss: 0.3439 - val_accuracy: 0.6315\n",
            "Epoch 36/100\n",
            "125/125 [==============================] - 5s 40ms/step - loss: 0.1836 - accuracy: 0.7138 - val_loss: 0.3462 - val_accuracy: 0.6285\n",
            "Epoch 37/100\n",
            "125/125 [==============================] - 5s 40ms/step - loss: 0.1818 - accuracy: 0.7161 - val_loss: 0.3440 - val_accuracy: 0.6310\n",
            "Epoch 38/100\n",
            "125/125 [==============================] - 5s 39ms/step - loss: 0.1801 - accuracy: 0.7192 - val_loss: 0.3470 - val_accuracy: 0.6292\n",
            "Epoch 39/100\n",
            "125/125 [==============================] - 5s 39ms/step - loss: 0.1785 - accuracy: 0.7210 - val_loss: 0.3449 - val_accuracy: 0.6314\n",
            "Epoch 40/100\n",
            "125/125 [==============================] - 5s 40ms/step - loss: 0.1768 - accuracy: 0.7241 - val_loss: 0.3450 - val_accuracy: 0.6317\n",
            "Epoch 41/100\n",
            "125/125 [==============================] - 5s 40ms/step - loss: 0.1752 - accuracy: 0.7268 - val_loss: 0.3455 - val_accuracy: 0.6333\n",
            "Epoch 42/100\n",
            "125/125 [==============================] - 5s 40ms/step - loss: 0.1737 - accuracy: 0.7286 - val_loss: 0.3439 - val_accuracy: 0.6345\n",
            "Epoch 43/100\n",
            "125/125 [==============================] - 5s 39ms/step - loss: 0.1721 - accuracy: 0.7314 - val_loss: 0.3459 - val_accuracy: 0.6333\n",
            "Epoch 44/100\n",
            "125/125 [==============================] - 5s 39ms/step - loss: 0.1719 - accuracy: 0.7316 - val_loss: 0.3465 - val_accuracy: 0.6324\n",
            "Epoch 45/100\n",
            "125/125 [==============================] - 5s 39ms/step - loss: 0.1694 - accuracy: 0.7354 - val_loss: 0.3468 - val_accuracy: 0.6338\n",
            "Epoch 46/100\n",
            "125/125 [==============================] - 5s 39ms/step - loss: 0.1679 - accuracy: 0.7380 - val_loss: 0.3506 - val_accuracy: 0.6297\n",
            "Epoch 47/100\n",
            "125/125 [==============================] - 5s 40ms/step - loss: 0.1664 - accuracy: 0.7399 - val_loss: 0.3507 - val_accuracy: 0.6299\n",
            "Epoch 48/100\n",
            "125/125 [==============================] - 5s 39ms/step - loss: 0.1650 - accuracy: 0.7426 - val_loss: 0.3487 - val_accuracy: 0.6339\n",
            "Epoch 49/100\n",
            "125/125 [==============================] - 5s 40ms/step - loss: 0.1636 - accuracy: 0.7442 - val_loss: 0.3486 - val_accuracy: 0.6354\n",
            "Epoch 50/100\n",
            "125/125 [==============================] - 5s 39ms/step - loss: 0.1623 - accuracy: 0.7468 - val_loss: 0.3502 - val_accuracy: 0.6335\n",
            "Epoch 51/100\n",
            "125/125 [==============================] - 5s 40ms/step - loss: 0.1609 - accuracy: 0.7489 - val_loss: 0.3507 - val_accuracy: 0.6321\n",
            "Epoch 52/100\n",
            "125/125 [==============================] - 5s 40ms/step - loss: 0.1597 - accuracy: 0.7507 - val_loss: 0.3542 - val_accuracy: 0.6295\n",
            "Epoch 53/100\n",
            "125/125 [==============================] - 5s 40ms/step - loss: 0.1583 - accuracy: 0.7528 - val_loss: 0.3570 - val_accuracy: 0.6288\n",
            "Epoch 54/100\n",
            "125/125 [==============================] - 5s 40ms/step - loss: 0.1569 - accuracy: 0.7555 - val_loss: 0.3566 - val_accuracy: 0.6294\n",
            "Epoch 55/100\n",
            "125/125 [==============================] - 5s 39ms/step - loss: 0.1557 - accuracy: 0.7572 - val_loss: 0.3556 - val_accuracy: 0.6331\n",
            "Epoch 56/100\n",
            "125/125 [==============================] - 5s 39ms/step - loss: 0.1544 - accuracy: 0.7593 - val_loss: 0.3613 - val_accuracy: 0.6259\n",
            "Epoch 57/100\n",
            "125/125 [==============================] - 5s 39ms/step - loss: 0.1532 - accuracy: 0.7609 - val_loss: 0.3603 - val_accuracy: 0.6296\n",
            "Epoch 58/100\n",
            "125/125 [==============================] - 5s 39ms/step - loss: 0.1523 - accuracy: 0.7621 - val_loss: 0.3599 - val_accuracy: 0.6299\n",
            "Epoch 59/100\n",
            "125/125 [==============================] - 5s 40ms/step - loss: 0.1509 - accuracy: 0.7648 - val_loss: 0.3613 - val_accuracy: 0.6275\n",
            "Epoch 60/100\n",
            "125/125 [==============================] - 5s 40ms/step - loss: 0.1496 - accuracy: 0.7668 - val_loss: 0.3608 - val_accuracy: 0.6322\n",
            "Epoch 61/100\n",
            "125/125 [==============================] - 5s 40ms/step - loss: 0.1484 - accuracy: 0.7687 - val_loss: 0.3645 - val_accuracy: 0.6270\n",
            "Epoch 62/100\n",
            "125/125 [==============================] - 5s 39ms/step - loss: 0.1473 - accuracy: 0.7706 - val_loss: 0.3655 - val_accuracy: 0.6290\n",
            "Epoch 63/100\n",
            "125/125 [==============================] - 5s 39ms/step - loss: 0.1461 - accuracy: 0.7727 - val_loss: 0.3699 - val_accuracy: 0.6235\n",
            "Epoch 64/100\n",
            "125/125 [==============================] - 5s 40ms/step - loss: 0.1450 - accuracy: 0.7740 - val_loss: 0.3683 - val_accuracy: 0.6258\n",
            "Epoch 65/100\n",
            "125/125 [==============================] - 5s 40ms/step - loss: 0.1440 - accuracy: 0.7759 - val_loss: 0.3705 - val_accuracy: 0.6253\n",
            "Epoch 66/100\n",
            "125/125 [==============================] - 5s 41ms/step - loss: 0.1428 - accuracy: 0.7774 - val_loss: 0.3693 - val_accuracy: 0.6274\n",
            "Epoch 67/100\n",
            "125/125 [==============================] - 5s 40ms/step - loss: 0.1416 - accuracy: 0.7798 - val_loss: 0.3741 - val_accuracy: 0.6250\n",
            "Epoch 68/100\n",
            "125/125 [==============================] - 5s 40ms/step - loss: 0.1406 - accuracy: 0.7808 - val_loss: 0.3799 - val_accuracy: 0.6208\n",
            "Epoch 69/100\n",
            "125/125 [==============================] - 5s 40ms/step - loss: 0.1394 - accuracy: 0.7832 - val_loss: 0.3785 - val_accuracy: 0.6223\n",
            "Epoch 70/100\n",
            "125/125 [==============================] - 5s 40ms/step - loss: 0.1385 - accuracy: 0.7844 - val_loss: 0.3819 - val_accuracy: 0.6208\n",
            "Epoch 71/100\n",
            "125/125 [==============================] - 5s 40ms/step - loss: 0.1374 - accuracy: 0.7866 - val_loss: 0.3798 - val_accuracy: 0.6223\n",
            "Epoch 72/100\n",
            "125/125 [==============================] - 5s 40ms/step - loss: 0.1363 - accuracy: 0.7886 - val_loss: 0.3804 - val_accuracy: 0.6228\n",
            "Epoch 73/100\n",
            "125/125 [==============================] - 5s 40ms/step - loss: 0.1354 - accuracy: 0.7903 - val_loss: 0.3818 - val_accuracy: 0.6221\n",
            "Epoch 74/100\n",
            "125/125 [==============================] - 5s 40ms/step - loss: 0.1343 - accuracy: 0.7912 - val_loss: 0.3871 - val_accuracy: 0.6192\n",
            "Epoch 75/100\n",
            "125/125 [==============================] - 5s 40ms/step - loss: 0.1333 - accuracy: 0.7930 - val_loss: 0.3853 - val_accuracy: 0.6215\n",
            "Epoch 76/100\n",
            "125/125 [==============================] - 5s 39ms/step - loss: 0.1325 - accuracy: 0.7938 - val_loss: 0.3864 - val_accuracy: 0.6237\n",
            "Epoch 77/100\n",
            "125/125 [==============================] - 5s 39ms/step - loss: 0.1314 - accuracy: 0.7959 - val_loss: 0.3882 - val_accuracy: 0.6226\n",
            "Epoch 78/100\n",
            "125/125 [==============================] - 5s 39ms/step - loss: 0.1305 - accuracy: 0.7976 - val_loss: 0.3934 - val_accuracy: 0.6181\n",
            "Epoch 79/100\n",
            "125/125 [==============================] - 5s 39ms/step - loss: 0.1295 - accuracy: 0.7995 - val_loss: 0.3932 - val_accuracy: 0.6223\n",
            "Epoch 80/100\n",
            "125/125 [==============================] - 5s 40ms/step - loss: 0.1285 - accuracy: 0.8007 - val_loss: 0.3962 - val_accuracy: 0.6190\n",
            "Epoch 81/100\n",
            "125/125 [==============================] - 5s 39ms/step - loss: 0.1277 - accuracy: 0.8018 - val_loss: 0.4000 - val_accuracy: 0.6145\n",
            "Epoch 82/100\n",
            "125/125 [==============================] - 5s 40ms/step - loss: 0.1268 - accuracy: 0.8036 - val_loss: 0.4025 - val_accuracy: 0.6159\n",
            "Epoch 83/100\n",
            "125/125 [==============================] - 5s 40ms/step - loss: 0.1260 - accuracy: 0.8049 - val_loss: 0.3982 - val_accuracy: 0.6200\n",
            "Epoch 84/100\n",
            "125/125 [==============================] - 5s 40ms/step - loss: 0.1250 - accuracy: 0.8062 - val_loss: 0.4001 - val_accuracy: 0.6187\n",
            "Epoch 85/100\n",
            "125/125 [==============================] - 5s 40ms/step - loss: 0.1241 - accuracy: 0.8076 - val_loss: 0.4022 - val_accuracy: 0.6166\n",
            "Epoch 86/100\n",
            "125/125 [==============================] - 5s 40ms/step - loss: 0.1232 - accuracy: 0.8086 - val_loss: 0.4047 - val_accuracy: 0.6177\n",
            "Epoch 87/100\n",
            "125/125 [==============================] - 5s 39ms/step - loss: 0.1224 - accuracy: 0.8111 - val_loss: 0.4109 - val_accuracy: 0.6123\n",
            "Epoch 88/100\n",
            "125/125 [==============================] - 5s 40ms/step - loss: 0.1215 - accuracy: 0.8120 - val_loss: 0.4074 - val_accuracy: 0.6164\n",
            "Epoch 89/100\n",
            "125/125 [==============================] - 5s 40ms/step - loss: 0.1207 - accuracy: 0.8131 - val_loss: 0.4138 - val_accuracy: 0.6147\n",
            "Epoch 90/100\n",
            "125/125 [==============================] - 5s 40ms/step - loss: 0.1199 - accuracy: 0.8144 - val_loss: 0.4102 - val_accuracy: 0.6153\n",
            "Epoch 91/100\n",
            "125/125 [==============================] - 5s 40ms/step - loss: 0.1191 - accuracy: 0.8160 - val_loss: 0.4140 - val_accuracy: 0.6161\n",
            "Epoch 92/100\n",
            "125/125 [==============================] - 5s 39ms/step - loss: 0.1183 - accuracy: 0.8169 - val_loss: 0.4199 - val_accuracy: 0.6128\n",
            "Epoch 93/100\n",
            "125/125 [==============================] - 5s 39ms/step - loss: 0.1175 - accuracy: 0.8182 - val_loss: 0.4163 - val_accuracy: 0.6173\n",
            "Epoch 94/100\n",
            "125/125 [==============================] - 5s 39ms/step - loss: 0.1167 - accuracy: 0.8196 - val_loss: 0.4214 - val_accuracy: 0.6135\n",
            "Epoch 95/100\n",
            "125/125 [==============================] - 5s 40ms/step - loss: 0.1161 - accuracy: 0.8204 - val_loss: 0.4178 - val_accuracy: 0.6140\n",
            "Epoch 96/100\n",
            "125/125 [==============================] - 5s 39ms/step - loss: 0.1151 - accuracy: 0.8218 - val_loss: 0.4225 - val_accuracy: 0.6158\n",
            "Epoch 97/100\n",
            "125/125 [==============================] - 5s 39ms/step - loss: 0.1145 - accuracy: 0.8232 - val_loss: 0.4245 - val_accuracy: 0.6120\n",
            "Epoch 98/100\n",
            "125/125 [==============================] - 5s 39ms/step - loss: 0.1138 - accuracy: 0.8240 - val_loss: 0.4289 - val_accuracy: 0.6111\n",
            "Epoch 99/100\n",
            "125/125 [==============================] - 5s 39ms/step - loss: 0.1130 - accuracy: 0.8253 - val_loss: 0.4300 - val_accuracy: 0.6094\n",
            "Epoch 100/100\n",
            "125/125 [==============================] - 5s 39ms/step - loss: 0.1123 - accuracy: 0.8266 - val_loss: 0.4290 - val_accuracy: 0.6138\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as lstm_cell_layer_call_fn, lstm_cell_layer_call_and_return_conditional_losses, lstm_cell_1_layer_call_fn, lstm_cell_1_layer_call_and_return_conditional_losses, lstm_cell_layer_call_fn while saving (showing 5 of 10). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: s2s\\assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: s2s\\assets\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vfqVXplH5mU3"
      },
      "source": [
        "## Run inference (sampling)\n",
        "\n",
        "1. encode input and retrieve initial decoder state\n",
        "2. run one step of decoder with this initial state\n",
        "and a \"start of sequence\" token as target.\n",
        "Output will be the next target token.\n",
        "3. Repeat with the current target token and current states\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E0tZcHQdk6hP"
      },
      "source": [
        "# Reverse-lookup token index to decode sequences back to\n",
        "# something readable.\n",
        "reverse_input_char_index = dict((i, char) for char, i in input_token_index.items())\n",
        "reverse_target_char_index = dict((i, char) for char, i in target_token_index.items())\n",
        "\n",
        "def decode_sequence(input_seq):\n",
        "    # Encode the input as state vectors.\n",
        "    _, states_value = encoder_model.predict(input_seq)\n",
        "\n",
        "    # Generate empty target sequence of length 1.\n",
        "    target_seq = np.zeros((1, 1))\n",
        "    # Populate the first character of target sequence with the start character.\n",
        "    target_seq[0, 0] = target_token_index[\"\\t\"]\n",
        "\n",
        "    # Sampling loop for a batch of sequences\n",
        "    # (to simplify, here we assume a batch of size 1).\n",
        "    stop_condition = False\n",
        "    decoded_sentence = \"\"\n",
        "    while not stop_condition:\n",
        "        output_tokens, [h, c] = decoder_model.predict([target_seq , states_value])\n",
        "\n",
        "        # Sample a token\n",
        "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
        "        sampled_char = reverse_target_char_index[sampled_token_index]\n",
        "        decoded_sentence += sampled_char\n",
        "\n",
        "        # Exit condition: either hit max length\n",
        "        # or find stop character.\n",
        "        if sampled_char == \"\\n\" or len(decoded_sentence) > max_decoder_seq_length:\n",
        "            stop_condition = True\n",
        "\n",
        "        # Update the target sequence (of length 1).\n",
        "        target_seq = np.zeros((1, 1))\n",
        "        target_seq[0, 0] = sampled_token_index\n",
        "\n",
        "        # Update states\n",
        "        states_value = [h, c]\n",
        "    return decoded_sentence\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XSXGe9l15mU3"
      },
      "source": [
        "You can now generate decoded sentences as such:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mUkjboqu5mU4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3a85fb72-fce5-4de2-bd5b-ae4974dae815"
      },
      "source": [
        "for seq_index in range(20):\n",
        "    # Take one sequence (part of the training set)\n",
        "    # for trying out decoding.\n",
        "    input_seq = encoder_input_data[seq_index : seq_index + 1]\n",
        "    decoded_sentence = decode_sequence(input_seq)\n",
        "    print(\"-\")\n",
        "    print(\"Input sentence:\", input_texts[seq_index])\n",
        "    print(\"Decoded sentence:\", decoded_sentence)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "-\n",
            "Input sentence: It seems unlikely that the train will arrive on time.\n",
            "Decoded sentence: Il se passe dans la pièce et le train était arrivé de l'école.\n",
            "\n",
            "-\n",
            "Input sentence: It was much more difficult than we initially thought.\n",
            "Decoded sentence: Ce fut la semaine de la maison comportement la plus grosse erreur.\n",
            "\n",
            "-\n",
            "Input sentence: It was much more difficult than we initially thought.\n",
            "Decoded sentence: Ce fut la semaine de la maison comportement la plus grosse erreur.\n",
            "\n",
            "-\n",
            "Input sentence: It was much more difficult than we initially thought.\n",
            "Decoded sentence: Ce fut la semaine de la maison comportement la plus grosse erreur.\n",
            "\n",
            "-\n",
            "Input sentence: It was much more difficult than we initially thought.\n",
            "Decoded sentence: Ce fut la semaine de la maison comportement la plus grosse erreur.\n",
            "\n",
            "-\n",
            "Input sentence: It was so hot that I thought I was going to pass out.\n",
            "Decoded sentence: Il faisait tellement froid que tu ne sais pas simpler le temps de rentrer chez moi.\n",
            "\n",
            "-\n",
            "Input sentence: It was so hot that I thought I was going to pass out.\n",
            "Decoded sentence: Il faisait tellement froid que tu ne sais pas simpler le temps de rentrer chez moi.\n",
            "\n",
            "-\n",
            "Input sentence: It was so still that you would have heard a pin drop.\n",
            "Decoded sentence: C'était la cour était difficile de comprendre la main de ce que j'ai dites.\n",
            "\n",
            "-\n",
            "Input sentence: It was such a cold day that we decided not to go out.\n",
            "Decoded sentence: Il faisait tellement froid que tu ne sais pas si satifiant.\n",
            "\n",
            "-\n",
            "Input sentence: It will take time for him to recover from his wounds.\n",
            "Decoded sentence: Ça lui prendra d'une fois que je voulais assurer à ma maison.\n",
            "\n",
            "-\n",
            "Input sentence: It would be a shame to let all this food go to waste.\n",
            "Decoded sentence: Ce serait dommage de vous rendre ce qui il trente à Boston.\n",
            "\n",
            "-\n",
            "Input sentence: It would be better for you not to ask him for advice.\n",
            "Decoded sentence: Ça serait mort de ne pas vous aider à toute ce qui vous le fait.\n",
            "\n",
            "-\n",
            "Input sentence: It'll probably take you about three hours to do that.\n",
            "Decoded sentence: Cela me produit de la maison de cette maison avec moi de manière à notre chambre.\n",
            "\n",
            "-\n",
            "Input sentence: It's a nice day, isn't it? Why not go out for a walk?\n",
            "Decoded sentence: C'est une bonne idée de quelle commerçait la maison avant la douver.\n",
            "\n",
            "-\n",
            "Input sentence: It's about time you were independent of your parents.\n",
            "Decoded sentence: Il n'est pas rien d'autre que le temps prochaine de la manière de passer.\n",
            "\n",
            "-\n",
            "Input sentence: It's been a long time since I've had a real vacation.\n",
            "Decoded sentence: Cela fait un parcontrôle devenu ici à la maison des gens.\n",
            "\n",
            "-\n",
            "Input sentence: It's been a long time since I've seen Tom this happy.\n",
            "Decoded sentence: Cela fait le chien de temps à l'air de tels avec lui.\n",
            "\n",
            "-\n",
            "Input sentence: It's difficult to feel at home in a foreign language.\n",
            "Decoded sentence: C'est difficile d'aider des gens qui ne vont pas tenir camper avec nous.\n",
            "\n",
            "-\n",
            "Input sentence: It's hard for an old man to change his way of living.\n",
            "Decoded sentence: C'est difficile d'aider des gens qui ne vont pas tenir camper avec nous.\n",
            "\n",
            "-\n",
            "Input sentence: It's hotter here in the valley than in the mountains.\n",
            "Decoded sentence: Il est impossible d'un point de passer sa vieille vie sardi.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "biidqSL0JPJm"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}